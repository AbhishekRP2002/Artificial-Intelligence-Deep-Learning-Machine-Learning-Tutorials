{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtPv0MY+E74u3TtGUuAP63",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f67c14e9df584dcb8a1f2cf5a6c10da5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7bc73e21a9b47aa8463651995d6bb58",
              "IPY_MODEL_79bbc599eeae4ab3b49f6af0baf29773",
              "IPY_MODEL_3f382173a4114ed58db8cac3fa544861"
            ],
            "layout": "IPY_MODEL_e59c88fe25ae4b3e9c95b236f3ef1b64"
          }
        },
        "b7bc73e21a9b47aa8463651995d6bb58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f893ad3fb2b84240903ea884b10602c5",
            "placeholder": "​",
            "style": "IPY_MODEL_b52949e30e994c06ae32549ffbc924ae",
            "value": "100%"
          }
        },
        "79bbc599eeae4ab3b49f6af0baf29773": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4861d60943c47a88ac9ad072af6c7fe",
            "max": 23,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c83aac7e1c24e7dbc5c6c312732bb57",
            "value": 23
          }
        },
        "3f382173a4114ed58db8cac3fa544861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_969814aa15e6471aa39f26858a2a44c2",
            "placeholder": "​",
            "style": "IPY_MODEL_d5e167b626584031b99b154c4e7ed688",
            "value": " 23/23 [00:12&lt;00:00,  1.76it/s]"
          }
        },
        "e59c88fe25ae4b3e9c95b236f3ef1b64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f893ad3fb2b84240903ea884b10602c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b52949e30e994c06ae32549ffbc924ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4861d60943c47a88ac9ad072af6c7fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c83aac7e1c24e7dbc5c6c312732bb57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "969814aa15e6471aa39f26858a2a44c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5e167b626584031b99b154c4e7ed688": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhishekRP2002/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials/blob/master/HN_Q%26A_using_SearchAPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p56KBDGRfCQw",
        "outputId": "7ee07666-affc-4091-e137-8795c1c646cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vuuHCrk_xp-o"
      },
      "outputs": [],
      "source": [
        "# Dependencies\n",
        "from datetime import date, timedelta  # date handling for fetching recent news\n",
        "from IPython import display  # for pretty printing\n",
        "import json  # for parsing the JSON api responses and model outputs\n",
        "from numpy import dot  # for cosine similarity\n",
        "import openai  # for using GPT and getting embeddings\n",
        "import os  # for loading environment variables\n",
        "import requests  # for making the API requests\n",
        "from tqdm.notebook import tqdm  # for printing progress bars\n",
        "\n",
        "# Load environment variables\n",
        "news_api_key = os.getenv(\"NEWS_API_KEY\")\n",
      
        "\n",
        "GPT_MODEL = \"gpt-3.5-turbo\"\n",
        "\n",
        "\n",
        "# Helper functions\n",
        "def json_gpt(input: str):\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=GPT_MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Your are an intelligent AI Programmer. Your goal is to Output only valid JSON\"},\n",
        "            {\"role\": \"user\", \"content\": input},\n",
        "        ],\n",
        "        temperature=0.5,\n",
        "    )\n",
        "\n",
        "    text = completion.choices[0].message.content\n",
        "    parsed = json.loads(text)\n",
        "\n",
        "    return parsed\n",
        "\n",
        "\n",
        "def embeddings(input: list[str]) -> list[list[str]]:\n",
        "    response = openai.Embedding.create(model=\"text-embedding-ada-002\", input=input)\n",
        "    return [data.embedding for data in response.data]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# User asks a question\n",
        "USER_QUESTION = \"How can I use language models for my AI start up ?\""
      ],
      "metadata": {
        "id": "gRMkOUu7x-kj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QUERIES_INPUT = f\"\"\"\n",
        "You have access to a search API that returns recent news articles and information.\n",
        "Generate an array of search queries that are relevant to this question.\n",
        "Use a variation of related keywords for the queries, trying to be as general as possible.\n",
        "Include as many queries as you can think of, including and excluding terms.\n",
        "For example, include queries like ['keyword_1 keyword_2', 'keyword_1', 'keyword_2'].\n",
        "Be creative. The more queries you include, the more likely you are to find relevant results.\n",
        "\n",
        "User question: {USER_QUESTION}\n",
        "\n",
        "Format: {{\"queries\": [\"query_1\", \"query_2\", \"query_3\"]}}\n",
        "\"\"\"\n",
        "\n",
        "queries = json_gpt(QUERIES_INPUT)[\"queries\"]\n",
        "\n",
        "# Let's include the original question as well for good measure\n",
        "queries.append(USER_QUESTION)\n",
        "\n",
        "queries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtRVsMU8yZ-v",
        "outputId": "9ae6c19a-4c3e-4a03-8e43-df8234b15296"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['language models AI start up',\n",
              " 'using language models for AI start up',\n",
              " 'language models for AI startups',\n",
              " 'incorporating language models in AI start ups',\n",
              " 'applying language models to AI start ups',\n",
              " 'language models and AI start ups',\n",
              " 'utilizing language models in AI start ups',\n",
              " 'leveraging language models for AI start ups',\n",
              " 'language models for AI companies',\n",
              " 'using language models for AI companies',\n",
              " 'language models and AI companies',\n",
              " 'incorporating language models in AI companies',\n",
              " 'applying language models to AI companies',\n",
              " 'utilizing language models in AI companies',\n",
              " 'leveraging language models for AI companies',\n",
              " 'language models for AI businesses',\n",
              " 'using language models for AI businesses',\n",
              " 'language models and AI businesses',\n",
              " 'incorporating language models in AI businesses',\n",
              " 'applying language models to AI businesses',\n",
              " 'utilizing language models in AI businesses',\n",
              " 'leveraging language models for AI businesses',\n",
              " 'How can I use language models for my AI start up ?']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def search_hacker_news(query: str, num_articles: int = 50, sort_by: str = \"relevance\", page: int = 0) -> dict:\n",
        "    base_url = \"http://hn.algolia.com/api/v1/search\"\n",
        "    params = {\n",
        "        \"query\": query,\n",
        "        \"page\": page,\n",
        "    }\n",
        "\n",
        "    if sort_by == \"relevance\":\n",
        "        params[\"tags\"] = \"story\"\n",
        "    elif sort_by == \"date\":\n",
        "        params[\"tags\"] = \"story\"\n",
        "        params[\"numericFilters\"] = \"created_at_i>0\"\n",
        "        params[\"sortBy\"] = \"created_at\"\n",
        "\n",
        "    response = requests.get(base_url, params=params)\n",
        "\n",
        "    return response.json()\n"
      ],
      "metadata": {
        "id": "q6RfyyNty1Os"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles = []\n",
        "\n",
        "for query in tqdm(queries):\n",
        "    result = search_hacker_news(query, num_articles=50, sort_by=\"relevance\")\n",
        "    if \"hits\" in result:\n",
        "        articles += result[\"hits\"]\n",
        "    else:\n",
        "        raise Exception(\"No hits found in the response.\")\n",
        "\n",
        "# Remove duplicates based on the \"objectID\" field\n",
        "unique_articles = {article[\"objectID\"]: article for article in articles}.values()\n",
        "\n",
        "print(\"Total number of articles:\", len(unique_articles))\n",
        "print(\"Top 5 articles of query 1:\", \"\\n\")\n",
        "\n",
        "for article in list(unique_articles)[:5]:\n",
        "    print(\"Title:\", article[\"title\"])\n",
        "    print(\"URL:\", article[\"url\"])\n",
        "    print(\"Points:\", article[\"points\"])\n",
        "    print(\"Number of Comments:\", article[\"num_comments\"])\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557,
          "referenced_widgets": [
            "f67c14e9df584dcb8a1f2cf5a6c10da5",
            "b7bc73e21a9b47aa8463651995d6bb58",
            "79bbc599eeae4ab3b49f6af0baf29773",
            "3f382173a4114ed58db8cac3fa544861",
            "e59c88fe25ae4b3e9c95b236f3ef1b64",
            "f893ad3fb2b84240903ea884b10602c5",
            "b52949e30e994c06ae32549ffbc924ae",
            "d4861d60943c47a88ac9ad072af6c7fe",
            "5c83aac7e1c24e7dbc5c6c312732bb57",
            "969814aa15e6471aa39f26858a2a44c2",
            "d5e167b626584031b99b154c4e7ed688"
          ]
        },
        "id": "ol5K-3fp0xPB",
        "outputId": "16a6010b-ba5a-44f9-c5f8-77562f0e6ce0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/23 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f67c14e9df584dcb8a1f2cf5a6c10da5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of articles: 50\n",
            "Top 5 articles of query 1: \n",
            "\n",
            "Title: Ask HN: Fastest way to get funding for a startup with the queue of early users?\n",
            "URL: None\n",
            "Points: 2\n",
            "Number of Comments: 1\n",
            "\n",
            "Title: Ask HN: How do I thrive in an AI dominated future?\n",
            "URL: None\n",
            "Points: 32\n",
            "Number of Comments: 55\n",
            "\n",
            "Title: Ask HN: Are Large Language Models Like GPT-3 a Hype?\n",
            "URL: None\n",
            "Points: 3\n",
            "Number of Comments: 6\n",
            "\n",
            "Title: Ask HN: Why doesn't Apple take AI more seriously?\n",
            "URL: None\n",
            "Points: 2\n",
            "Number of Comments: 2\n",
            "\n",
            "Title: Show HN: Retinello – An AI-Driven Learning Platform with Custom Context\n",
            "URL: https://retinello.com/\n",
            "Points: 1\n",
            "Number of Comments: 0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HA_INPUT = f\"\"\"\n",
        "Generate a hypothetical answer to the user's question. This answer will be used to rank search results.\n",
        "Pretend you have all the information you need to answer, but don't use any actual facts. Instead, use placeholders\n",
        "like NAME did something, or NAME said something at PLACE.\n",
        "\n",
        "User question: {USER_QUESTION}\n",
        "\n",
        "Format: {{\"hypotheticalAnswer\": \"hypothetical answer text\"}}\n",
        "\"\"\"\n",
        "\n",
        "hypothetical_answer = json_gpt(HA_INPUT)[\"hypotheticalAnswer\"]\n",
        "\n",
        "hypothetical_answer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "4D76F5TA02ZF",
        "outputId": "ceb0c17b-1c52-4469-ec1c-7e5056ff3c3a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You can use language models for your AI start up by incorporating them into your natural language processing algorithms. This will enable your AI system to understand and generate human-like text, enhancing its ability to communicate with users and provide more accurate and personalized responses. Additionally, language models can be used for tasks such as sentiment analysis, text summarization, and language translation, which can further enhance the capabilities of your AI start up.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hypothetical_answer_embedding = embeddings(hypothetical_answer)[0]\n",
        "article_embeddings = embeddings(\n",
        "    [\n",
        "        f\"{article['title']} {article['comment_text']} {article['story_text']}\"\n",
        "        for article in articles\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Calculate cosine similarity\n",
        "cosine_similarities = []\n",
        "for article_embedding in article_embeddings:\n",
        "    cosine_similarities.append(dot(hypothetical_answer_embedding, article_embedding))\n",
        "\n",
        "cosine_similarities[0:5]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BK3g9HPn1csb",
        "outputId": "91493845-55c2-4a1e-efc3-9d72fe57b011"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7837597776086492,\n",
              " 0.8381212926361241,\n",
              " 0.8348917042393856,\n",
              " 0.853983530121543,\n",
              " 0.7875939764093149,\n",
              " 0.7634125568767332,\n",
              " 0.8046866502043402,\n",
              " 0.7914060086584567,\n",
              " 0.7904447591200037,\n",
              " 0.8214736297243437]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scored_articles = zip(articles, cosine_similarities)\n",
        "\n",
        "# Sort articles by cosine similarity\n",
        "sorted_articles = sorted(scored_articles, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print top 5 articles\n",
        "print(\"Top 5 articles:\", \"\\n\")\n",
        "\n",
        "for article, score in sorted_articles[0:5]:\n",
        "    print(\"Title:\", article[\"title\"])\n",
        "    print(\"Description:\", article[\"story_text\"])\n",
        "    print(\"Comment:\", article[\"comment_text\"])\n",
        "    print(\"Score:\", score)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMmUtMB_1ksl",
        "outputId": "50df5669-d403-410b-da7c-8f6f4c144d1d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 articles: \n",
            "\n",
            "Title: Ask HN: What are you building with the current wave of AI and LLMs?\n",
            "Description: With the recent advancements in AI and large language models (LLMs) like GPT-4, I&#x27;m curious to learn what kind of projects you all have been working on utilizing these powerful tools.<p>It&#x27;s clear that we&#x27;re in the midst of a transformative period for AI, and the possibilities for its applications seem endless.<p>Personally, I&#x27;ve been experimenting with GPT-4 to create a platform that generates content for niche blogs, which has shown promising results so far. The ability of LLMs to understand context and generate relevant, high-quality content has made the process of content creation far more efficient.<p>Here are a few areas I&#x27;ve seen people leveraging AI and LLMs:<p>1. Personalized learning platforms: Utilizing AI to create tailored educational content for students based on their strengths and weaknesses.<p>2. Sentiment analysis: Companies analyzing customer reviews and social media posts to better understand their audience and improve their products and services.<p>3. Content summarization: Summarizing long articles and documents into concise summaries for easier consumption.<p>4. Creative writing and storytelling: Assisting authors and screenwriters in generating ideas, characters, and plots for their stories.<p>5. Medical diagnosis: Enhancing the diagnostic process by analyzing vast amounts of medical data and generating relevant insights for doctors.<p>These are just a few examples, and I&#x27;m sure there are many more interesting projects out there. So, please share your experiences and ideas!<p>* What projects are you currently working on or have completed that incorporate AI and LLMs?<p>* What challenges have you faced while working with these technologies?<p>* How do you think the landscape of AI and LLMs will evolve in the next few years?<p>Looking forward to hearing your thoughts and learning from your experiences!\n",
            "Comment: None\n",
            "Score: 0.8592063309558222\n",
            "\n",
            "Title: Ask HN: What are you building with the current wave of AI and LLMs?\n",
            "Description: With the recent advancements in AI and large language models (LLMs) like GPT-4, I&#x27;m curious to learn what kind of projects you all have been working on utilizing these powerful tools.<p>It&#x27;s clear that we&#x27;re in the midst of a transformative period for AI, and the possibilities for its applications seem endless.<p>Personally, I&#x27;ve been experimenting with GPT-4 to create a platform that generates content for niche blogs, which has shown promising results so far. The ability of LLMs to understand context and generate relevant, high-quality content has made the process of content creation far more efficient.<p>Here are a few areas I&#x27;ve seen people leveraging AI and LLMs:<p>1. Personalized learning platforms: Utilizing AI to create tailored educational content for students based on their strengths and weaknesses.<p>2. Sentiment analysis: Companies analyzing customer reviews and social media posts to better understand their audience and improve their products and services.<p>3. Content summarization: Summarizing long articles and documents into concise summaries for easier consumption.<p>4. Creative writing and storytelling: Assisting authors and screenwriters in generating ideas, characters, and plots for their stories.<p>5. Medical diagnosis: Enhancing the diagnostic process by analyzing vast amounts of medical data and generating relevant insights for doctors.<p>These are just a few examples, and I&#x27;m sure there are many more interesting projects out there. So, please share your experiences and ideas!<p>* What projects are you currently working on or have completed that incorporate AI and LLMs?<p>* What challenges have you faced while working with these technologies?<p>* How do you think the landscape of AI and LLMs will evolve in the next few years?<p>Looking forward to hearing your thoughts and learning from your experiences!\n",
            "Comment: None\n",
            "Score: 0.8591405956590575\n",
            "\n",
            "Title: Ask HN: What are you building with the current wave of AI and LLMs?\n",
            "Description: With the recent advancements in AI and large language models (LLMs) like GPT-4, I&#x27;m curious to learn what kind of projects you all have been working on utilizing these powerful tools.<p>It&#x27;s clear that we&#x27;re in the midst of a transformative period for AI, and the possibilities for its applications seem endless.<p>Personally, I&#x27;ve been experimenting with GPT-4 to create a platform that generates content for niche blogs, which has shown promising results so far. The ability of LLMs to understand context and generate relevant, high-quality content has made the process of content creation far more efficient.<p>Here are a few areas I&#x27;ve seen people leveraging AI and LLMs:<p>1. Personalized learning platforms: Utilizing AI to create tailored educational content for students based on their strengths and weaknesses.<p>2. Sentiment analysis: Companies analyzing customer reviews and social media posts to better understand their audience and improve their products and services.<p>3. Content summarization: Summarizing long articles and documents into concise summaries for easier consumption.<p>4. Creative writing and storytelling: Assisting authors and screenwriters in generating ideas, characters, and plots for their stories.<p>5. Medical diagnosis: Enhancing the diagnostic process by analyzing vast amounts of medical data and generating relevant insights for doctors.<p>These are just a few examples, and I&#x27;m sure there are many more interesting projects out there. So, please share your experiences and ideas!<p>* What projects are you currently working on or have completed that incorporate AI and LLMs?<p>* What challenges have you faced while working with these technologies?<p>* How do you think the landscape of AI and LLMs will evolve in the next few years?<p>Looking forward to hearing your thoughts and learning from your experiences!\n",
            "Comment: None\n",
            "Score: 0.8591405956590575\n",
            "\n",
            "Title: Ask HN: What are you building with the current wave of AI and LLMs?\n",
            "Description: With the recent advancements in AI and large language models (LLMs) like GPT-4, I&#x27;m curious to learn what kind of projects you all have been working on utilizing these powerful tools.<p>It&#x27;s clear that we&#x27;re in the midst of a transformative period for AI, and the possibilities for its applications seem endless.<p>Personally, I&#x27;ve been experimenting with GPT-4 to create a platform that generates content for niche blogs, which has shown promising results so far. The ability of LLMs to understand context and generate relevant, high-quality content has made the process of content creation far more efficient.<p>Here are a few areas I&#x27;ve seen people leveraging AI and LLMs:<p>1. Personalized learning platforms: Utilizing AI to create tailored educational content for students based on their strengths and weaknesses.<p>2. Sentiment analysis: Companies analyzing customer reviews and social media posts to better understand their audience and improve their products and services.<p>3. Content summarization: Summarizing long articles and documents into concise summaries for easier consumption.<p>4. Creative writing and storytelling: Assisting authors and screenwriters in generating ideas, characters, and plots for their stories.<p>5. Medical diagnosis: Enhancing the diagnostic process by analyzing vast amounts of medical data and generating relevant insights for doctors.<p>These are just a few examples, and I&#x27;m sure there are many more interesting projects out there. So, please share your experiences and ideas!<p>* What projects are you currently working on or have completed that incorporate AI and LLMs?<p>* What challenges have you faced while working with these technologies?<p>* How do you think the landscape of AI and LLMs will evolve in the next few years?<p>Looking forward to hearing your thoughts and learning from your experiences!\n",
            "Comment: None\n",
            "Score: 0.8591405956590575\n",
            "\n",
            "Title: Ask HN: AI Commercial Appications\n",
            "Description: Now that the dust has settled a bit, can people point to current concrete business end use cases of &quot;AI&quot; - specifically large language models? I&#x27;m aware of the kind of things they <i>can</i> do, I&#x27;m just curious now to see if that&#x27;s translated into actual use cases. To be clear, I&#x27;m talking about end uses, not &quot;selling picks and shovels in a gold rush&quot;.\n",
            "Comment: None\n",
            "Score: 0.8565792981977045\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_top_results = [\n",
        "    {\n",
        "        \"title\": article[\"title\"],\n",
        "        \"description\": article[\"story_text\"],\n",
        "        \"comment\": article[\"comment_text\"],\n",
        "    }\n",
        "    for article, _score in sorted_articles[0:]\n",
        "]\n",
        "\n",
        "ANSWER_INPUT = f\"\"\"\n",
        "Generate an answer to the user's question based on the given search results.\n",
        "TOP_RESULTS: {formatted_top_results}\n",
        "USER_QUESTION: {USER_QUESTION}\n",
        "\n",
        "Include as much information as possible in the answer. Reference the relevant search result urls as markdown links.\n",
        "\"\"\"\n",
        "\n",
        "completion = openai.ChatCompletion.create(\n",
        "    model=GPT_MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": ANSWER_INPUT}],\n",
        "    temperature=0.5,\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "text = \"\"\n",
        "for chunk in completion:\n",
        "    text += chunk.choices[0].delta.get(\"content\", \"\")\n",
        "    display.clear_output(wait=True)\n",
        "    display.display(display.Markdown(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "t-MITQvg6dCX",
        "outputId": "dd14e984-15d8-42fd-e6d1-6bc1e2f184ec"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-18346b848074>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \"\"\"\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m completion = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGPT_MODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mANSWER_INPUT\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Rate limit reached for default-gpt-3.5-turbo in organization org-tTVO6QZ0CYjF01N1ABuFjJuM on tokens per min. Limit: 90000 / min. Current: 0 / min. Contact us through our help center at help.openai.com if you continue to have issues."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U3a0cgpg78cu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
